{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reddit Comments Data into Parquet <a class=\"tocSkip\">\n",
    "This notebook loads the raw [Reddit comments dataset](http://academictorrents.com/details/85a5bd50e4c365f8df70240ffd4ecc7dec59912b) into a parquet file format. It does augment the data with several improved time columns, and the partitions the data by year/month/day. The file paths in this notebook should be modified for your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T08:34:31.416045Z",
     "start_time": "2022-09-15T08:34:30.146765Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.utils as U\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"RedditCommentsLoadToParquet\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T08:34:31.427088Z",
     "start_time": "2022-09-15T08:34:31.420675Z"
    }
   },
   "outputs": [],
   "source": [
    "reddit_comments_schema =  T.StructType([\n",
    "    T.StructField(\"id\", T.StringType()),\n",
    "    T.StructField(\"parent_id\", T.StringType()),\n",
    "    T.StructField(\"author\", T.StringType()),\n",
    "    T.StructField(\"link_id\", T.StringType()),\n",
    "    T.StructField(\"subreddit\", T.StringType()),\n",
    "    T.StructField(\"subreddit_id\", T.StringType()),\n",
    "    T.StructField(\"edited\", T.BooleanType()),\n",
    "    T.StructField(\"score\", T.LongType()),\n",
    "    T.StructField(\"body\", T.StringType()),\n",
    "    T.StructField(\"created_utc\", T.LongType()),\n",
    "    T.StructField(\"retrieved_utc\", T.LongType()),\n",
    "    T.StructField(\"retrieved_on\", T.LongType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T08:47:13.192762Z",
     "start_time": "2022-09-15T08:34:32.598344Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n",
    "def has_column(df, col_name):\n",
    "    if col_name in df.columns:\n",
    "        return F.lit(True)\n",
    "    else:\n",
    "        return F.lit(False)\n",
    "\n",
    "load_months = [\n",
    "#     (2021, 7),\n",
    "#     (2021, 8),\n",
    "#     (2021, 9),\n",
    "#     (2021, 10),\n",
    "#     (2021, 11),\n",
    "#     (2021, 12),\n",
    "#     (2022, 1),\n",
    "#     (2022, 2),\n",
    "#     (2022, 3),\n",
    "#     (2022, 4),\n",
    "    (2022, 8),\n",
    "]\n",
    "\n",
    "for year, month in load_months:\n",
    "    file_path = 'qfs:///data/reddit/comments/raw/RC_{0}-{1:02d}*.bz2'.format(year, month)\n",
    "    print('loading data for year-month {0}-{1:02d} at file path {2}'.format(year, month, file_path))\n",
    "    reddit_df = (\n",
    "        spark.read.json(\n",
    "            file_path,\n",
    "            schema=reddit_comments_schema,\n",
    "        )\n",
    "        .withColumn(\n",
    "            'retrieved_on',\n",
    "            F.when(\n",
    "                F.col('retrieved_utc').isNotNull(),\n",
    "                F.col('retrieved_utc')\n",
    "            ).otherwise(\n",
    "                F.col('retrieved_on')\n",
    "            )\n",
    "        )\n",
    "    )         \n",
    "\n",
    "    reddit_finalized = (\n",
    "        reddit_df\n",
    "        .select(\n",
    "            'author',\n",
    "             'link_id',\n",
    "            'retrieved_on',\n",
    "            'subreddit',\n",
    "            'subreddit_id',\n",
    "            'id',\n",
    "            'parent_id',\n",
    "            'edited',\n",
    "            'score',\n",
    "            'body',\n",
    "            'created_utc',\n",
    "            F.from_unixtime('created_utc', 'yyyy-MM-dd').alias('created_date'),\n",
    "            F.from_unixtime('created_utc', 'dd').alias('day')\n",
    "        )\n",
    "        .repartition('day')\n",
    "    ).cache()\n",
    "    print('    There are {0} total rows in month data set.'.format(reddit_finalized.count()))\n",
    "\n",
    "    out_path = 'qfs:///data/reddit/comments/processed/year={0}/month={1:02d}'.format(year, month)\n",
    "    print('    writing to: {0}'.format(out_path))\n",
    "    reddit_finalized.write.partitionBy(\n",
    "        'day'\n",
    "    ).parquet(\n",
    "        out_path,\n",
    "        mode='overwrite'\n",
    "    )\n",
    "    print('\\n')\n",
    "    reddit_finalized.unpersist()\n",
    "    del reddit_finalized\n",
    "    del reddit_df\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T08:47:43.932518Z",
     "start_time": "2022-09-15T08:47:13.195656Z"
    }
   },
   "outputs": [],
   "source": [
    "reddit_processed = spark.read.parquet('qfs:///data/reddit/comments/processed/')\n",
    "reddit_processed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T08:54:46.366672Z",
     "start_time": "2022-09-15T08:47:43.935354Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    reddit_processed\n",
    "    .groupBy('year')\n",
    "    .agg(\n",
    "        F.count('*').alias('count'),\n",
    "        F.countDistinct('author').alias('authors')\n",
    "    )\n",
    "    .orderBy('year')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T16:12:27.139557Z",
     "start_time": "2022-08-12T16:06:08.767951Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    reddit_processed\n",
    "    .groupBy('year')\n",
    "    .agg(\n",
    "        F.count('*').alias('count'),\n",
    "        F.countDistinct('author').alias('authors')\n",
    "    )\n",
    "    .orderBy('year')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-15T14:39:31.024042Z",
     "start_time": "2022-09-15T14:39:16.984361Z"
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    reddit_processed\n",
    "    .filter(\n",
    "        (F.col('year') == 2022)\n",
    "        &(F.col('month') == 8)\n",
    "    )\n",
    "    .groupBy('year','month','day')\n",
    "    .agg(\n",
    "        F.count('*').alias('count'),\n",
    "        F.countDistinct('author').alias('authors')\n",
    "    )\n",
    "    .orderBy('year','month','day')\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "217px",
    "width": "201px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
