{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Bot Commenters <a class=\"tocSkip\">\n",
    "Identifies likely bot commenters on Reddit using Benford's Law. See [original blog post](https://diybigdata.net/2020/03/using-benfords-law-to-identify-bots-on-reddit/) for a discussion on this technique.\n",
    "\n",
    "The core of this code is the `generateBenfordsLawAnalysis()` function, which takes a user event log data frame that must have a user ID column and a event timestamp column, and it returns the chi squared score of close each user's activity is to the ideal Benford's Law distribution. Scores closer to zero mean the user's activity more closely adheres to the ideal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:58:46.627964Z",
     "start_time": "2022-08-12T17:58:45.607762Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.window import Window as W\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"RedditBotCommenters\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:58:46.636108Z",
     "start_time": "2022-08-12T17:58:46.630656Z"
    }
   },
   "outputs": [],
   "source": [
    "orig_suffle_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:59:26.166948Z",
     "start_time": "2022-08-12T17:58:46.638715Z"
    }
   },
   "outputs": [],
   "source": [
    "reddit_df = (\n",
    "    spark.read.parquet('qfs:///data/reddit/comments/processed')\n",
    "    # filter out moderator and deleted authors\n",
    "    .filter(~F.col('author').isin('[deleted]','AutoModerator'))\n",
    ")\n",
    "\n",
    "reddit_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:59:42.896570Z",
     "start_time": "2022-08-12T17:59:26.168915Z"
    }
   },
   "outputs": [],
   "source": [
    "submissions_df = spark.read.parquet('qfs:///data/reddit/submissions/processed')\n",
    "submissions_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:59:42.969137Z",
     "start_time": "2022-08-12T17:59:42.899315Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_df = (\n",
    "    reddit_df\n",
    "    .select(\n",
    "        'author',\n",
    "        'created_utc',\n",
    "    )\n",
    "    .union(\n",
    "        submissions_df\n",
    "        .select(\n",
    "            'author',\n",
    "            'created_utc',\n",
    "        )\n",
    "    \n",
    "    )\n",
    "    .filter(\n",
    "        F.col('author').isNotNull()\n",
    "        &(F.length(F.col('author')) > 0)\n",
    "    )\n",
    "    .repartition('author')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`generateBenfordsLawAnalysis`\n",
    "\n",
    "A function to perform Benford's Law analysis against a data frame of user activities in order to determine which user's activities best (or least) adhere to the Benford's Law distribution. The data frame is ostensibly a event log keyed by a user ID and has a timestamp for each event row. Only the user ID and timesamps columns are used for analysis.\n",
    "\n",
    "### Arguments <a class=\"tocSkip\">\n",
    "* `df` - The data frame with the timestamped user activity to be analyzed\n",
    "* `user_col` - a string identifying the name of the column of df that contains the user IDs\n",
    "* `timestamp_col` - a string identifying the name of the column of df that contains the event timestamps. Must be `T.LongType()`.\n",
    "* `event_threshold` - the minimum number of events a user must have for the Benford's Law analysis to performed on it. Defaults to 100.\n",
    "\n",
    "### Returns <a class=\"tocSkip\">\n",
    "A dataframe with the following columns:\n",
    "* `user_col` - The user IDs. The column name will be the same as the original dataframe.\n",
    "* `frequency_count` - the number of events found for the user\n",
    "* `chi_squared` - the chi squared score indicating how similar the user's activity is to the ideal Benford's Law distribution.\n",
    "* `digit_share` - A list containing the relative share each first digit has among the user's activity. The list is ordered from digit 1 to digit 9.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T17:59:42.989415Z",
     "start_time": "2022-08-12T17:59:42.972762Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from math import log10, sqrt\n",
    "\n",
    "def _getUsersAndDigit(df, user_col, event_threshold):\n",
    "    digits_df = (\n",
    "        spark\n",
    "        .createDataFrame(\n",
    "            [[1], [2], [3], [4], [5], [6], [7], [8], [9]],\n",
    "            schema=T.StructType([\n",
    "                T.StructField(\n",
    "                    \"first_digit\", \n",
    "                    T.IntegerType()\n",
    "                )\n",
    "            ])\n",
    "        )\n",
    "        .coalesce(1)\n",
    "    )\n",
    "    users_and_digits = (\n",
    "        df\n",
    "        .groupBy(user_col)\n",
    "        .agg(F.count('*').alias('count'))\n",
    "        .filter(F.col('count') > event_threshold )\n",
    "        .select(user_col)\n",
    "        .repartition(user_col)\n",
    "        .crossJoin(digits_df)\n",
    "    )\n",
    "    return users_and_digits\n",
    "\n",
    "def _generateFirstDigitShare(df, user_col, timestamp_col):\n",
    "    user_event_window = W.partitionBy(user_col).orderBy(timestamp_col)\n",
    "    user_cum_dist_window = W.partitionBy(user_col).orderBy('first_digit')\n",
    "    \n",
    "    event_time_delta = F.col(timestamp_col) - F.lag(F.col(timestamp_col)).over(user_event_window)\n",
    "\n",
    "    first_digit_share = (\n",
    "        df\n",
    "        .select(\n",
    "            user_col,\n",
    "            timestamp_col,\n",
    "            event_time_delta.alias('time_delta')\n",
    "        )\n",
    "        .filter(F.col('time_delta').isNotNull())\n",
    "        .withColumn(\n",
    "            'first_digit',\n",
    "            F.substring(F.col('time_delta').cast(T.StringType()), 0, 1).cast(T.IntegerType())\n",
    "        )\n",
    "        .withColumn(\n",
    "            'first_digit_cum_dist',\n",
    "            F.cume_dist().over(user_cum_dist_window)\n",
    "        )\n",
    "        .groupBy(user_col, 'first_digit', 'first_digit_cum_dist')\n",
    "        .agg(\n",
    "            F.count(timestamp_col).alias('frequency_count')\n",
    "        )\n",
    "        .withColumn(\n",
    "            'first_digit_share',\n",
    "            F.col('first_digit_cum_dist') \n",
    "                - F.coalesce(\n",
    "                    F.lag('first_digit_cum_dist').over(user_cum_dist_window), \n",
    "                    F.lit(0)\n",
    "                )\n",
    "        )\n",
    "        .repartition(user_col)\n",
    "    )\n",
    "    return first_digit_share\n",
    "\n",
    "def _expectedBenfordsShare():\n",
    "    digits = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    expected_share_list = [(d, log10(d+1)-log10(d)) for d in digits]\n",
    "\n",
    "    expected_share_df = (\n",
    "        spark\n",
    "        .createDataFrame(\n",
    "            expected_share_list,\n",
    "            schema=T.StructType([\n",
    "                T.StructField(\n",
    "                    'first_digit', \n",
    "                    T.IntegerType()\n",
    "                ),\n",
    "                T.StructField(\n",
    "                    'expected_share',\n",
    "                    T.DoubleType()\n",
    "                )\n",
    "            ])\n",
    "        )\n",
    "        .coalesce(1)\n",
    "    )\n",
    "    \n",
    "    return expected_share_df\n",
    "\n",
    "def generateBenfordsLawAnalysis(df, user_col, timestamp_col, event_threshold = 100):\n",
    "    user_digts_df = _getUsersAndDigit(df, user_col, event_threshold)\n",
    "    first_digit_share_df = _generateFirstDigitShare(df, user_col, timestamp_col)\n",
    "    expected_share_df = _expectedBenfordsShare()\n",
    "    \n",
    "    finalized_first_digit_share_df = (\n",
    "        first_digit_share_df\n",
    "        .join(\n",
    "            user_digts_df,\n",
    "            on=[user_col,'first_digit'],\n",
    "            how='right'\n",
    "        )\n",
    "        .na.fill(0)\n",
    "        .cache()\n",
    "    )    \n",
    "    user_benford_distances = (\n",
    "        finalized_first_digit_share_df\n",
    "        .join(\n",
    "            F.broadcast(expected_share_df),\n",
    "            on='first_digit',\n",
    "            how='inner'\n",
    "        )\n",
    "        .withColumn(\n",
    "            'chi_squared_addends',\n",
    "            F.pow(\n",
    "                (F.col('first_digit_share') - F.col('expected_share')),\n",
    "                F.lit(2)\n",
    "            ) / F.col('expected_share')\n",
    "        )\n",
    "        .orderBy(user_col, 'first_digit')\n",
    "        .groupBy(user_col)\n",
    "        .agg(\n",
    "            F.sum('frequency_count').alias('frequency_count'),\n",
    "            F.sum('chi_squared_addends').alias('chi_squared'),\n",
    "            F.collect_list(F.col('first_digit_share')).alias('digit_share')\n",
    "        )\n",
    "    )\n",
    "    return user_benford_distances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T18:23:56.132897Z",
     "start_time": "2022-08-12T17:59:42.991016Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df = generateBenfordsLawAnalysis(reddit_df, 'author', 'created_utc')\n",
    "\n",
    "new_df.orderBy(F.col('chi_squared').desc()).limit(50).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-12T20:16:38.977709Z",
     "start_time": "2022-08-12T20:15:52.691420Z"
    }
   },
   "outputs": [],
   "source": [
    "new_df.write.parquet(\n",
    "    'qfs:///user/spark/reddit/author_bot_chi_squared_score/',\n",
    "    mode='overwrite'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
